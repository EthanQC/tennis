# README.md

- 本文档是针对网球识别后端相关功能进行解读  

主要流程是：服务器和平板连接到同一局域网内，服务器启动后台，然后不断处理视频，处理的流程是：

1. 收到app的socket连接请求后启动，每n分钟（目前是16s）**截取**一个帧率100的视频片段
2. 将16s的视频**降采样**为帧率16的视频
3. 对分割后的片段**逐个**调用模型处理，处理完毕生成轨迹图和记录轨迹的文档，发送落点图片、轨迹图和视频片段到app
4. app收到资源后展示视频和落点图片，如果对某个时间段的落点有争议，点击该落点图片，发送**仲裁**到服务器端
5. 服务器端收到仲裁调用找出该落点帧在原来100帧视频的那一帧，将原视频这一帧前后的60帧交给模型处理，处理完毕生成运动场地的全景图，如果识别到球落地的帧，描绘出球落点和边线，以判断是否出界，同时还会专门截取全景图球附近部分的图以进一步判断。这三个图和这60帧的100帧回放视频会发送到app以展示。
6. 注意所有请求都应该是异步的。

## HTTP请求

routes.py下编写的是HTTP请求的相关接口

### '/video'

该接口功能是实时播放摄像头拍摄的视频，便于调整摄像头  
一打开app就会发送'/video'的接口，所以就在这个接口内**初始化**摄像头  
实现功能的方式是捕获摄像头获得的每一帧图片，并以**视频流**的形式返回给安卓端  

### '/cleanUp'

该接口发送于客户端退出时，用于释放摄像头资源

## SocketIo

在project.py文件下编写了两个socketio接口

### namespace = '/predict'

该接口是用于接收安卓端发送的信息(mode),处理了两个模式： 

  1. 测试模式下，直接将一个视频传递给TennisV3下的d3.py文件进行处理，得到降采集之后的视频路径以及落地帧对应在视频中的第几帧（一个视频可能有**多个**落地帧，所以接收到的是一个int类型的list）  
  2. 实时模式下，采用了多线程的处理方式，一个线程用于录制视频，并将所有帧的信息存入一个list中，并将这个list存入一个queue中，方便另一个线程获取(**生产者消费者模式**),同时使用摄像头厂商提供的sdk将视频存入'TennisV3/ori_video/segment_{segment_number}.avi'中  
第二个线程是用于将100帧的视频降采样成一个**16帧**的视频，先从queue中取出一个帧的list，交给TennisV3下的d3.py分析视频得到16帧的视频路径和视频的落地帧list，最后用sendVideo()向app返回视频和落地帧
  3. sendVideo()是用于将获取到的视频和落地帧返回给安卓端。首先将视频转化成base64编码格式，并且以**字符串**的形式存储(文件名`*`视频内容)(可以改成json)。然后打开视频，根据算法返回的落地帧信息，取出该帧的图片并且编码为base64，同样以**字符串**的形式存储(文件名`*`视频内容)(可以改成json)，这里的两个字符串都是存在一个字符串中的，结果返回这个字符串给安卓端

### namespace = '/mapping''

该接口用于实现仲裁的功能  

1. 接收安卓端发送过来的特定落地帧信息(**图片的文件名去掉后缀，不包含.号**)
2. 将该帧对应的原来的视频路径和该帧对应的位置传入TennisV3下的d3.py文件中，得到该帧在原100帧视频前后的60帧视频，以及运动场地的全景图，如果识别到球落地的帧，描绘出球落点和边线，和截取下来的全景图球附近部分的图，三张图片
3. 最后将这三张通过send_map_result()返回给前端
4. send_map_result大致的格式接受格式跟前面send_video相同，也是以*号间隔：

视频名\*视频内容[\*图片1名\*图片1内容

（\*图片名1.1\*图片1.1内容）

（\*图片名1.2\*图片1.2内容）

（\*图片名1.3\*图片1.3内容）

]……重复[]部分

每个落地帧都固定包含3张结果图
